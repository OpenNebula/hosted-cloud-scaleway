---
ansible_python_interpreter: /usr/bin/python3
ansible_ssh_private_key_file: "{{ inventory_dir }}/../scw/003.opennebula_instances/opennebula.pem"

validation:

  # TEST CASE NAME: Core Services
  # DESCRIPTION: Check the status of OpenNebula core services, as listed in `service_list`,
  # also tests the resilience of some of them by restarting. 
  # OUTPUT: HTML document at /tmp/cloud_verification_report.html
  run_core_services: true
  core_services:
    service_list:
      - name: opennebula.service
        desc: OpenNebula core (oned)
      - name: opennebula-gate.service
        desc: OpenNebula gate
      - name: opennebula-flow.service
        desc: OpenNebula flow
      # NOTE: Scheduler service is always checked in earlier than 6.99 versions
      # - name: opennebula-scheduler.service
      #   desc: OpenNebula Scheduler
      - name: opennebula-fireedge.service
        desc: OpenNebula Fireedge GUI
      # Add other services if more should be checked.
    check_fireedge_ui: true

  # TEST CASE NAME: Storage Benchmark
  # DESCRIPTION: Instantiates a VM for running storage benchmark from it. The VM needs to
  # have public internet access to install dependencies. The VM's connected network can
  # be configured.
  # OUTPUT: HTML document at /tmp/cloud_verification_report.html
  run_storage_benchmark: true
  storage_benchmark:
    vnet_name: vxlan2

  # TEST CASE NAME: Network Benchmark
  # DESCRIPTION: Runs an iperf bandwidth and ping benchmark between all hypervisor hosts
  # that are listed in the inventory.
  # OUTPUT: Ansible execution output in the command line
  run_network_benchmark: true
  network_benchmark:
    iperf_port: 5201
    iperf_test_time: 10

  # TEST CASE NAME: Connectivity Matrix
  # DESCRIPTION: Instantiates a test VM on each hypervisor hosts registered in the 
  # OpenNebula cloud. From each host accesses the VM (temporarily sets up local routing
  # on the configured bridge) and runs a connectivity test to all other VMs running on
  # the other hosts.
  # OUTPUT: 
  #    - HTML document at /tmp/conn-matrix-report.html
  #    - Raw JSON data at /tmp/conn-matrix-raw-data.json
  run_conn_matrix: true
  conn_matrix:
    bridge_name: br0
    ping_count: 10
    vnet_name: test2 

  # TEST CASE NAME: VM instantiation
  # DESCRIPTION: Fetches a VM template and image from the OpenNebula Marketplace and instantiates it.
  # Optionally also instantiates a VNET, attaches it to the VM, and checks connectivity.
  # OUTPUT: HTML document at /tmp/cloud_verification_report.html
  run_test_vm: true
  test_vm:
    vm:
      check_connection: true
      market_name: 'Alpine Linux 3.21'
      template_extra: |
        MEMORY="512"
    create_vnet: true
    vnet:
      name: 'vxlan2'
      desc: 'A test network for post-deployment cloud verification'
      bridge: 'br0'
      vn_mad: 'bridge'
      ipam_mad: 'scw-flexip'
      phydev: 'enp5s0'
      network_address: '62.210.100.79'
      network_mask: '255.255.255.255'
      gateway: '62.210.0.1'
      dns: '51.159.47.28'
      ar:
        - type: "IP4"
          ip: '62.210.100.79'
          size: '1'
          mac: '52:54:00:77:61:22'

  # TEST CASE NAME: VM High Availability
  # DESCRIPTION: The prerequisite is to have VM HA configured, see details in OpenNebula documentation.
  # Instantiates a VM and produces an error on its host with a configurable method (e.g. bringing the 
  # main interface down). Checks that the error is detected and the VM is migrated to another host.
  # NOTE: Blocks the test execution and asks for manual confirmation before the fencing occurs (that will shut
  # down the host). Does not provide an automatic way to recover the host.
  # OUTPUT: HTML document at /tmp/cloud_verification_report.html
  run_vm_ha: false
  vm_ha:
    produce_error_method: 'if_down'
    if_down_params:
      interface_name: 'br-test'
    fencing_check_retries: 8
    fencing_check_delay: 60

  # TEST CASE NAME: Front-end High Availability
  # DESCRIPTION: The prerequisite is to have the Front-end HA configured, see details in OpenNebula documentation.
  # Checks that config file contents are the same on all FEs, checks the selected leader, simulates a leader failover
  # by stopping the opennebula service and verifies that a new leader is selected.
  # OUTPUT: HTML document at /tmp/fe_ha_report.html
  run_fe_ha: false
  fe_ha:
    one_config_path:
      - /etc/one
      - /var/lib/one/remotes/etc
    one_zone_name: OpenNebula
